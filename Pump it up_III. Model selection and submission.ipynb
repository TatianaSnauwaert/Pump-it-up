{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Pump it Up: Data Mining the Water Table\n",
    "\n",
    "### Can you predict which water pumps are faulty?\n",
    "\n",
    "Using data from Taarifa and the Tanzanian Ministry of Water, we are trying to predict which pumps are functional, which need some repairs, and which don't work at all based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed. \n",
    "\n",
    "A smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler as ss\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# machine learning\n",
    "#Trees    \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "\n",
    "#Ensemble Methods\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # explicitly require this experimental feature\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier # now you can import normally from ensemble\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Gaussian Processes\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "    \n",
    "#GLM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.linear_model import Perceptron   \n",
    "    \n",
    "#Nearest Neighbor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "    \n",
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import NuSVC\n",
    "    \n",
    "# xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "#Discriminant Analysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    " #Navies Bayes\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# PCA\n",
    "from sklearn import decomposition\n",
    "\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path of the CSV file to read\n",
    "train_df_final = pd.read_csv(\"train_df_final.csv\")\n",
    "X_test_final = pd.read_csv(\"X_test_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14850, 30)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59400, 31)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building\n",
    "## Train/test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df_final.drop(\"label\",axis=1)\n",
    "y = train_df_final[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify.\n",
    "\n",
    "For example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y will make sure that your random split has 25% of 0's and 75% of 1's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaling\n",
    "\n",
    "The idea behind StandardScaler is that it will transform your data such that its distribution will have a mean value 0 and standard deviation of 1.\n",
    "\n",
    "In case of multivariate data, this is done feature-wise (in other words independently for each column of the data).\n",
    "\n",
    "Given the distribution of the data, each value in the dataset will have the mean value subtracted, and then divided by the standard deviation of the whole dataset (or feature in the multivariate case).\n",
    "\n",
    "https://stackoverflow.com/questions/40758562/can-anyone-explain-me-standardscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = ss()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_valid = sc.transform(X_valid)\n",
    "X_test = sc.transform(X_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle component analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an instance of the Model\n",
    "pca = decomposition.PCA(.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=0.95, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_valid_pca = pca.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "### Try different models\n",
    "\n",
    "**TO DO**: combine all models in a loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try K-nearest neighbors? If possible to use long and lat features to define areas like on the map above with hue = labels -> it's logical that pumps in the same area function similary...? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Common Model Algorithms\n",
    "# from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn import model_selection\n",
    "\n",
    "\n",
    "# #Machine Learning Algorithm (MLA) Selection and Initialization\n",
    "# MLA = [\n",
    "#     #Ensemble Methods\n",
    "#   #  ensemble.AdaBoostClassifier(),\n",
    "#     ensemble.BaggingClassifier(),\n",
    "#     ensemble.ExtraTreesClassifier(),\n",
    "#     ensemble.GradientBoostingClassifier(),\n",
    "#     ensemble.RandomForestClassifier(),\n",
    "\n",
    "#     #Gaussian Processes\n",
    "#     gaussian_process.GaussianProcessClassifier(),\n",
    "    \n",
    "#     #GLM\n",
    "#     linear_model.LogisticRegressionCV(),\n",
    "#     linear_model.PassiveAggressiveClassifier(),\n",
    "#     linear_model.RidgeClassifierCV(),\n",
    "#     linear_model.SGDClassifier(),\n",
    "#     linear_model.Perceptron(),\n",
    "    \n",
    "#     #Navies Bayes\n",
    "#     naive_bayes.BernoulliNB(),\n",
    "#     naive_bayes.GaussianNB(),\n",
    "    \n",
    "#     #Nearest Neighbor\n",
    "#     neighbors.KNeighborsClassifier(),\n",
    "    \n",
    "#     #SVM\n",
    "#     svm.SVC(probability=True),\n",
    "#     svm.NuSVC(probability=True),\n",
    "#     svm.LinearSVC(),\n",
    "    \n",
    "#     #Trees    \n",
    "#     tree.DecisionTreeClassifier(),\n",
    "#     tree.ExtraTreeClassifier(),\n",
    "    \n",
    "#     #Discriminant Analysis\n",
    "#     discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "#     discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "\n",
    "    \n",
    "#     #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "#     XGBClassifier()    \n",
    "#     ]\n",
    "\n",
    "# #split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n",
    "# #note: this is an alternative to train_test_split\n",
    "# cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .7, random_state = 0 ) # run model 10x with 70/30 split \n",
    "\n",
    "# #create table to compare MLA metrics\n",
    "# MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\n",
    "# MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "# # #create table to compare MLA predictions\n",
    "# # MLA_predict = y\n",
    "\n",
    "# #index through MLA and save performance to table\n",
    "# row_index = 0\n",
    "# for alg in MLA:\n",
    "\n",
    "#     #set name and parameters\n",
    "#     MLA_name = alg.__class__.__name__\n",
    "#     MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "#     MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    \n",
    "#     #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "#     cv_results = model_selection.cross_validate(alg, X, y, cv  = cv_split,return_train_score=True)\n",
    "\n",
    "#     MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "#     MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "#     MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n",
    "#     #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n",
    "#     MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n",
    "    \n",
    "\n",
    "# #     #save MLA predictions - see section 6 for usage\n",
    "# #     alg.fit(X, y)\n",
    "# #     MLA_predict[MLA_name] = alg.predict(X)\n",
    "    \n",
    "#     row_index+=1\n",
    "\n",
    "    \n",
    "# #print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\n",
    "# MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "# MLA_compare\n",
    "# #MLA_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred = decision_tree.predict(X_valid)\n",
    "\n",
    "acc_decision_tree = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "acc_decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Tree\n",
    "\n",
    "extra_tree = DecisionTreeClassifier()\n",
    "extra_tree.fit(X_train, y_train)\n",
    "y_pred = extra_tree.predict(X_valid)\n",
    "\n",
    "acc_extra_tree = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "acc_extra_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.19"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "rfc = RandomForestClassifier(criterion='entropy', n_estimators = 1000,min_samples_split=8,random_state=42,verbose=5)\n",
    "rfc.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred = rfc.predict(X_valid_pca)\n",
    "\n",
    "acc_rfc = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "acc_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1       39072.4027           36.35m\n",
      "         2       36456.2492           37.71m\n",
      "         3       34245.8275           37.64m\n",
      "         4       32326.4488           37.41m\n",
      "         5       30635.3955           37.00m\n",
      "         6       29119.2526           36.79m\n",
      "         7       27733.2958           36.50m\n",
      "         8       26499.9202           36.11m\n",
      "         9       25380.9488           35.65m\n",
      "        10       24348.9807           35.33m\n",
      "        11       23418.3368           34.93m\n",
      "        12       22565.3432           34.49m\n",
      "        13       21742.2631           34.20m\n",
      "        14       20984.1346           34.09m\n",
      "        15       20275.8523           33.72m\n",
      "        16       19636.4608           33.29m\n",
      "        17       19032.3979           32.85m\n",
      "        18       18472.9692           32.36m\n",
      "        19       17961.3823           31.83m\n",
      "        20       17457.6397           31.39m\n",
      "        21       16970.5176           30.96m\n",
      "        22       16552.5084           30.47m\n",
      "        23       16138.8318           29.98m\n",
      "        24       15760.6724           29.44m\n",
      "        25       15408.8567           28.91m\n",
      "        26       15080.6894           28.38m\n",
      "        27       14777.5286           27.82m\n",
      "        28       14456.6570           27.35m\n",
      "        29       14174.9988           26.83m\n",
      "        30       13898.4751           26.35m\n",
      "        31       13649.6629           25.82m\n",
      "        32       13413.8326           25.28m\n",
      "        33       13200.8078           24.71m\n",
      "        34       13001.6322           24.17m\n",
      "        35       12796.9283           23.64m\n",
      "        36       12617.5950           23.09m\n",
      "        37       12451.3762           22.54m\n",
      "        38       12282.4589           22.03m\n",
      "        39       12101.1869           21.62m\n",
      "        40       11959.7481           21.10m\n",
      "        41       11820.5742           20.59m\n",
      "        42       11697.9250           20.07m\n",
      "        43       11575.4431           19.56m\n",
      "        44       11458.6325           19.06m\n",
      "        45       11349.8966           18.58m\n",
      "        46       11250.5710           18.08m\n",
      "        47       11131.1187           17.62m\n",
      "        48       11038.7351           17.15m\n",
      "        49       10921.2708           16.72m\n",
      "        50       10832.2934           16.27m\n",
      "        51       10756.6125           15.80m\n",
      "        52       10682.9847           15.35m\n",
      "        53       10610.3206           14.91m\n",
      "        54       10534.7032           14.49m\n",
      "        55       10458.4474           14.06m\n",
      "        56       10392.2260           13.63m\n",
      "        57       10317.4301           13.23m\n",
      "        58       10257.7320           12.82m\n",
      "        59       10200.6881           12.42m\n",
      "        60       10141.2246           12.02m\n",
      "        61       10072.6172           11.65m\n",
      "        62       10008.8491           11.27m\n",
      "        63        9937.6234           10.91m\n",
      "        64        9889.8737           10.53m\n",
      "        65        9839.5986           10.17m\n",
      "        66        9790.6413            9.81m\n",
      "        67        9750.2546            9.45m\n",
      "        68        9697.3537            9.10m\n",
      "        69        9648.8314            8.76m\n",
      "        70        9599.4115            8.42m\n",
      "        71        9534.3211            8.10m\n",
      "        72        9493.7885            7.77m\n",
      "        73        9434.5830            7.46m\n",
      "        74        9394.5702            7.13m\n",
      "        75        9353.7229            6.82m\n",
      "        76        9309.9045            6.51m\n",
      "        77        9276.1531            6.20m\n",
      "        78        9233.5058            5.89m\n",
      "        79        9175.2000            5.60m\n",
      "        80        9146.7749            5.30m\n",
      "        81        9109.3403            5.01m\n",
      "        82        9075.8615            4.72m\n",
      "        83        9035.1939            4.43m\n",
      "        84        8995.0993            4.15m\n",
      "        85        8951.6754            3.87m\n",
      "        86        8923.0239            3.59m\n",
      "        87        8898.0856            3.31m\n",
      "        88        8872.7315            3.04m\n",
      "        89        8834.4294            2.78m\n",
      "        90        8802.3602            2.51m\n",
      "        91        8777.8140            2.25m\n",
      "        92        8741.7207            1.99m\n",
      "        93        8705.6864            1.73m\n",
      "        94        8669.7579            1.48m\n",
      "        95        8646.8854            1.23m\n",
      "        96        8607.8106           58.61s\n",
      "        97        8582.8882           43.74s\n",
      "        98        8556.7222           29.02s\n",
      "        99        8530.1671           14.44s\n",
      "       100        8499.1594            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "77.04"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GradientBoostingClassifier\n",
    "\n",
    "GB = GradientBoostingClassifier(n_estimators=100, learning_rate=0.075, \n",
    "                                max_depth=14,max_features=1.0,\n",
    "                                min_samples_leaf=14, verbose=10)\n",
    "\n",
    "GB.fit(X_train_pca, y_train)     \n",
    "y_pred = GB.predict(X_valid_pca)\n",
    "\n",
    "acc_GB = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "acc_GB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.28"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Histogram-based Gradient Boosting Classification Tree.\n",
    "\n",
    "#This estimator is much faster than GradientBoostingClassifier for big datasets (n_samples >= 10 000).\n",
    "\n",
    "\n",
    "HGB = HistGradientBoostingClassifier(learning_rate=0.075, loss='categorical_crossentropy', \n",
    "                                               max_depth=8, min_samples_leaf=15)\n",
    "\n",
    "HGB = HGB.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred = HGB.predict(X_valid_pca)\n",
    "\n",
    "acc_HGB = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "acc_HGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75.98"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LightGBM \n",
    "\n",
    "#is another fast tree based gradient boosting algorithm, which supports GPU, and parallel learning.\n",
    "\n",
    "\n",
    "LGB = LGBMClassifier(objective='multiclass', learning_rate=0.75, num_iterations=100, \n",
    "                     num_leaves=40, random_state=123, max_depth=15)\n",
    "\n",
    "LGB.fit(X_train_pca, y_train)\n",
    "y_pred = LGB.predict(X_valid_pca)\n",
    "\n",
    "acc_LGB = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "acc_LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost classifier\n",
    "\n",
    "AB = AdaBoostClassifier(n_estimators=100, learning_rate=0.075)\n",
    "AB.fit(X_train, y_train)     \n",
    "y_pred = AB.predict(X_valid)\n",
    "\n",
    "acc_AB = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "acc_AB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BaggingClassifier\n",
    "\n",
    "BC = BaggingClassifier(n_estimators=100)\n",
    "BC.fit(X_train, y_train)     \n",
    "y_pred = BC.predict(X_valid)\n",
    "\n",
    "acc_BC = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "acc_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExtraTreesClassifier\n",
    "\n",
    "ETC = ExtraTreesClassifier(n_estimators=100)\n",
    "ETC.fit(X_train, y_train)     \n",
    "y_pred = ETC.predict(X_valid)\n",
    "\n",
    "acc_ETC = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "acc_ETC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression for multilabel classification\n",
    "\n",
    "# https://acadgild.com/blog/logistic-regression-multiclass-classification\n",
    "# https://medium.com/@jjw92abhi/is-logistic-regression-a-good-multi-class-classifier-ad20fecf1309\n",
    "\n",
    "LG = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\")\n",
    "LG.fit(X_train, y_train)     \n",
    "y_pred = LG.predict(X_valid)\n",
    "\n",
    "acc_LG = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "acc_LG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. This can be done by calculating the coefficient of the features in the decision function.\n",
    "\n",
    "Positive coefficients increase the odds of the response (and thus increase the probability), and negative coefficients decrease the odds of the response (and thus decrease the probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df = pd.DataFrame(train_df_final.columns.delete(0))\n",
    "coeff_df.columns = ['Feature']\n",
    "coeff_df[\"Correlation\"] = pd.Series(LG.coef_[0])\n",
    "\n",
    "coeff_df.sort_values(by='Correlation', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PassiveAggressiveClassifier\n",
    "\n",
    "PAC = PassiveAggressiveClassifier()\n",
    "PAC.fit(X_train, y_train)\n",
    "y_pred = PAC.predict(X_valid)\n",
    "\n",
    "acc_PAC = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "acc_PAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RidgeClassifierCV\n",
    "\n",
    "RC = RidgeClassifierCV()\n",
    "RC.fit(X_train, y_train)\n",
    "y_pred = RC.predict(X_valid)\n",
    "\n",
    "acc_RC = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "acc_RC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron\n",
    "\n",
    "P = Perceptron()\n",
    "P.fit(X_train, y_train)\n",
    "y_pred = P.predict(X_valid)\n",
    "\n",
    "acc_P = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "acc_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "# https://scikit-learn.org/stable/modules/sgd.html#implementation-details\n",
    "\n",
    "SGD = SGDClassifier(shuffle=True,average=True)\n",
    "SGD.fit(X_train, y_train)\n",
    "y_pred = SGD.predict(X_valid)\n",
    "\n",
    "acc_SGD = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "acc_SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_valid)\n",
    "\n",
    "acc_knn = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "acc_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Classifier\n",
    "\n",
    "SVC = SVC(probability=True)\n",
    "SVC.fit(X_train, y_train)\n",
    "y_pred = SVC.predict(X_valid)\n",
    "\n",
    "acc_SVC = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "acc_SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC\n",
    "\n",
    "linear_SVC = LinearSVC()\n",
    "linear_SVC.fit(X_train,y_train)\n",
    "linear_SVC.predict(X_valid)\n",
    "\n",
    "acc_linear_SVC = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "acc_linear_SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "\n",
    "# xgb = XGBClassifier(n_estimators=1000, learning_rate=0.05, n_jobs=5)\n",
    "# xgb.fit(X_train, y_train, \n",
    "#              early_stopping_rounds=5, \n",
    "#              eval_set=[(X_valid, y_valid)], \n",
    "#              verbose=False)\n",
    "\n",
    "# y_pred = xgb.predict(X_valid)\n",
    "# acc_xgb = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "# acc_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearDiscriminantAnalysis\n",
    "\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "LDA.fit(X_train,y_train)\n",
    "LDA.predict(X_valid)\n",
    "\n",
    "acc_LDA = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "acc_LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuadraticDiscriminantAnalysis\n",
    "\n",
    "QDA = QuadraticDiscriminantAnalysis()\n",
    "QDA.fit(X_train,y_train)\n",
    "QDA.predict(X_valid)\n",
    "\n",
    "acc_QDA = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "acc_QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BernoulliNB\n",
    "\n",
    "bernoulliNB = BernoulliNB()\n",
    "bernoulliNB.fit(X_train,y_train)\n",
    "bernoulliNB.predict(X_valid)\n",
    "\n",
    "acc_bernoulliNB = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "acc_bernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GaussianNB\n",
    "\n",
    "gaussianNB = GaussianNB()\n",
    "gaussianNB.fit(X_train,y_train)\n",
    "gaussianNB.predict(X_valid)\n",
    "\n",
    "acc_gaussianNB = round(accuracy_score(y_valid,y_pred) * 100, 2)\n",
    "acc_gaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Decision Tree',\"Extra Tree\",'Random Forest','Support Vector', 'KNN', 'Logistic Regression', \n",
    "              'Stochastic Gradient Decent', 'Linear SVC', \"Ada Boost Classifier\", \n",
    "              \"Bagging Classifier\", \"Passive Agressive Cl\", \"Ridge\",\"Perceptron\",\n",
    "              'Gradient Boosting Classifier',\"Extra Trees\",\n",
    "              \"LinearDA\",\"QuadraticDA\",\"BernoulliNB\",\"GaussianNB\"],\n",
    "    'Score': [acc_decision_tree,acc_extra_tree,acc_rfc, acc_SVC, acc_knn, acc_LG,\n",
    "              acc_SGD, acc_linear_SVC, acc_AB, \n",
    "              acc_BC, acc_PAC, acc_RC, acc_P,\n",
    "              acc_GB, acc_ETC,\n",
    "             acc_LDA, acc_QDA, acc_bernoulliNB, acc_gaussianNB]})\n",
    "sorted_by_score = models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\n",
    "sns.barplot(x='Score', y = 'Model', data = sorted_by_score, color = 'g')\n",
    "\n",
    "#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\n",
    "plt.title('Machine Learning Algorithm Accuracy Score \\n')\n",
    "plt.xlabel('Accuracy Score on validation data (%)')\n",
    "plt.ylabel('Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 3 models are: \n",
    "- Gradient Boosting Classifier\n",
    "- Random Forest\n",
    "- Bagging Classifier\n",
    "\n",
    "Out of them, the Random Forest is the fastest one.\n",
    "We are now going to find the best parameters for these 3 models using GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "We will be using the Grid Search for hyperparameter tuning here. We need to find the following best parameters for our Gradient Boosting model:\n",
    "- learning rate\n",
    "- max_depth\n",
    "- min_samples_leaf\n",
    "- max_featres\n",
    "- n_estimators\n",
    "\n",
    "In order to significantly reduce the massive computational cost of doing the brute force calculations, we will run the gridsearch on only two parameters at a time.\n",
    "Reference: https://zlatankr.github.io/posts/2017/01/23/pump-it-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!! Takes too long!\n",
    "# grid_search full\n",
    "# GB = GradientBoostingClassifier(n_estimators=100, \n",
    "#                                 learning_rate=0.075,\n",
    "#                                 max_depth=14,\n",
    "#                                 max_features=1.0,\n",
    "#                                 min_samples_leaf=16)\n",
    "\n",
    "\n",
    "# param_grid = {\"n_estimators\" : [50,100, 150],\n",
    "#               \"learning_rate\":[0.05, 0.025, 0.075, 0.01],\n",
    "#              \"max_depth\" : [12,13,14], \n",
    "#               \"min_samples_leaf\":[14,15,16,17],\n",
    "#              \"max_features\" : [0.5,0.3,0.7,1.0]}\n",
    "\n",
    "# gs = GridSearchCV(estimator=GB,\n",
    "#                   param_grid=param_grid,\n",
    "#                   scoring='accuracy',\n",
    "#                   cv=10,\n",
    "#                   n_jobs=-1)\n",
    "\n",
    "# gs.fit(X, y)\n",
    "\n",
    "# print(gs.best_score_)\n",
    "# print(gs.best_params_)\n",
    "# print(gs.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed: 22.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7936531986531986\n",
      "{'min_samples_split': 8, 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "# Tuning for RF\n",
    "sc = ss()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "rfc = RandomForestClassifier(criterion='entropy', n_estimators = 50,random_state=42)\n",
    "\n",
    "params = {\"min_samples_split\" : [4, 6, 8],\n",
    "             \"n_estimators\" : [500, 700, 1000]}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rfc, cv=4, param_grid=params, n_jobs=-1, verbose=5) # n_jobs=-1 = use all the CPU cores\n",
    "\n",
    "grid_search.fit(X, y.values.ravel())\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 108 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   48.9s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 13.1min\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed: 19.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7827609427609428\n",
      "{'learning_rate': 0.75, 'max_depth': 15, 'num_iterations ': 100, 'num_leaves': 40}\n"
     ]
    }
   ],
   "source": [
    "# Tuning for LGB\n",
    "sc = ss()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "LGB = LGBMClassifier(objective='multiclass', num_threads=2, verbose=2, random_state=123)\n",
    "\n",
    "params = {'num_iterations ': [100, 150, 200],\n",
    "          'max_depth': [5, 8, 15],\n",
    "          'learning_rate': [0.01, 0.75, 0.1, 0.2],\n",
    "          'num_leaves' : [25, 40, 50]\n",
    "         }\n",
    "\n",
    "grid_search = GridSearchCV(estimator=LGB, cv=4, param_grid=params, n_jobs=-1, verbose=5) # n_jobs=-1 = use all the CPU cores\n",
    "\n",
    "grid_search.fit(X, y.values.ravel())\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result:\n",
    "0.7936531986531986\n",
    "{'min_samples_split': 8, 'n_estimators': 1000}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Result:\n",
    "0.7827609427609428\n",
    "{'learning_rate': 0.75, 'max_depth': 15, 'num_iterations ': 100, 'num_leaves': 40}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomized search full\n",
    "GB = GradientBoostingClassifier(n_estimators=100, \n",
    "                                learning_rate=0.075,\n",
    "                                max_depth=14,\n",
    "                                max_features=1.0,\n",
    "                                min_samples_leaf=16)\n",
    "\n",
    "\n",
    "param_dist = {\"n_estimators\" : [50,100, 150],\n",
    "              \"learning_rate\":[0.05, 0.025, 0.075, 0.01],\n",
    "             \"max_depth\" : [12,13,14], \n",
    "              \"min_samples_leaf\":[14,15,16,17],\n",
    "             \"max_features\" : [0.5,0.3,0.7,1.0]}\n",
    "\n",
    "rs = RandomizedSearchCV(estimator=GB,\n",
    "                  param_distributions=param_dist,\n",
    "                  scoring='accuracy',\n",
    "                  cv=10, n_iter=10, n_jobs=-1)\n",
    "\n",
    "rs.fit(X, y)\n",
    "\n",
    "print(rs.best_score_)\n",
    "print(rs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomizedSearchCV result:\n",
    "\n",
    "param_dist \n",
    "- \"n_estimators\" : [50,100, 150],\n",
    "- \"learning_rate\":[0.05, 0.025, 0.075, 0.01],\n",
    "- \"max_depth\" : [12,13,14], \n",
    "- \"min_samples_leaf\":[14,15,16,17],\n",
    "- \"max_features\" : [0.5,0.3,0.7,1.0]\n",
    "\n",
    "0.7967676767676768\n",
    "\n",
    "{'n_estimators': 100, 'min_samples_leaf': 14, 'max_features': 0.5, 'max_depth': 14, 'learning_rate': 0.075}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomized search min_samples_split\n",
    "GB = GradientBoostingClassifier(n_estimators=100, \n",
    "                                learning_rate=0.075,\n",
    "                                max_depth=14,\n",
    "                                max_features=1.0,\n",
    "                                min_samples_leaf=14)\n",
    "\n",
    "\n",
    "param_dist = {\"min_samples_split\":[0.1,0.5,0.3,0.7,1.0]}\n",
    "\n",
    "rs = RandomizedSearchCV(estimator=GB,\n",
    "                  param_distributions=param_dist,\n",
    "                  scoring='accuracy',\n",
    "                  cv=10, n_iter=10, n_jobs=-1)\n",
    "\n",
    "rs.fit(X, y)\n",
    "\n",
    "print(rs.best_score_)\n",
    "print(rs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat((pd.DataFrame(X_df.columns, columns = ['variable']), \n",
    "           pd.DataFrame(GB.feature_importances_, columns = ['importance'])), \n",
    "          axis = 1).sort_values(by='importance', ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain the chosen model on the whole train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=8,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = ss()\n",
    "X = sc.fit_transform(X)\n",
    "X_test = sc.transform(X_test_final)\n",
    "\n",
    "rfc = RandomForestClassifier(criterion='gini',min_samples_split=8, n_estimators=1000)\n",
    "\n",
    "rfc.fit(X, y)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1       49195.3374           18.62m\n",
      "         2       46278.0771           19.49m\n",
      "         3       43821.1036           19.67m\n",
      "         4       41717.8833           19.72m\n",
      "         5       39836.5003           19.57m\n",
      "         6       38181.2752           19.36m\n",
      "         7       36675.6249           19.26m\n",
      "         8       35352.8606           19.13m\n",
      "         9       34057.2438           19.03m\n",
      "        10       32947.2188           18.81m\n",
      "        11       31921.2694           18.66m\n",
      "        12       31002.1300           18.50m\n",
      "        13       30179.6605           18.23m\n",
      "        14       29414.8409           17.95m\n",
      "        15       28698.2535           17.75m\n",
      "        16       28049.5921           17.47m\n",
      "        17       27410.8015           17.25m\n",
      "        18       26824.9707           16.98m\n",
      "        19       26267.6517           16.72m\n",
      "        20       25766.1268           16.45m\n",
      "        21       25320.6267           16.17m\n",
      "        22       24878.8274           15.92m\n",
      "        23       24443.8331           15.71m\n",
      "        24       24072.7444           15.41m\n",
      "        25       23704.9478           15.12m\n",
      "        26       23340.6308           14.85m\n",
      "        27       23056.9473           14.53m\n",
      "        28       22739.3599           14.26m\n",
      "        29       22462.1078           13.96m\n",
      "        30       22163.8596           13.70m\n",
      "        31       21899.8906           13.42m\n",
      "        32       21650.4412           13.15m\n",
      "        33       21390.8408           12.87m\n",
      "        34       21157.7781           12.59m\n",
      "        35       20934.2589           12.32m\n",
      "        36       20720.6422           12.06m\n",
      "        37       20538.6055           11.77m\n",
      "        38       20342.7950           11.50m\n",
      "        39       20160.6787           11.23m\n",
      "        40       19963.5959           10.99m\n",
      "        41       19810.8211           10.70m\n",
      "        42       19615.1206           10.47m\n",
      "        43       19475.5282           10.20m\n",
      "        44       19314.3446            9.95m\n",
      "        45       19172.9199            9.68m\n",
      "        46       19031.3983            9.44m\n",
      "        47       18874.6560            9.21m\n",
      "        48       18780.8421            8.94m\n",
      "        49       18676.6922            8.69m\n",
      "        50       18556.2117            8.46m\n",
      "        51       18443.6875            8.24m\n",
      "        52       18333.3428            8.01m\n",
      "        53       18244.1708            7.77m\n",
      "        54       18148.1958            7.55m\n",
      "        55       17993.5268            7.35m\n",
      "        56       17906.3858            7.15m\n",
      "        57       17795.9128            6.95m\n",
      "        58       17697.1863            6.74m\n",
      "        59       17592.5316            6.54m\n",
      "        60       17491.4494            6.37m\n",
      "        61       17403.5240            6.17m\n",
      "        62       17314.7463            5.97m\n",
      "        63       17259.2034            5.75m\n",
      "        64       17182.9308            5.56m\n",
      "        65       17111.6992            5.36m\n",
      "        66       17061.3760            5.17m\n",
      "        67       16987.0697            4.99m\n",
      "        68       16912.2995            4.82m\n",
      "        69       16820.6227            4.66m\n",
      "        70       16752.6027            4.47m\n",
      "        71       16688.5988            4.30m\n",
      "        72       16616.8458            4.12m\n",
      "        73       16575.0636            3.94m\n",
      "        74       16477.6365            3.78m\n",
      "        75       16432.7789            3.60m\n",
      "        76       16387.6573            3.43m\n",
      "        77       16289.5319            3.27m\n",
      "        78       16226.4168            3.11m\n",
      "        79       16152.3834            2.96m\n",
      "        80       16075.6512            2.80m\n",
      "        81       16032.1821            2.64m\n",
      "        82       15975.4919            2.49m\n",
      "        83       15898.6786            2.34m\n",
      "        84       15835.4487            2.19m\n",
      "        85       15783.8131            2.04m\n",
      "        86       15691.0968            1.90m\n",
      "        87       15626.3760            1.76m\n",
      "        88       15577.2135            1.61m\n",
      "        89       15514.4713            1.47m\n",
      "        90       15407.9838            1.34m\n",
      "        91       15355.4130            1.20m\n",
      "        92       15303.3685            1.06m\n",
      "        93       15243.3950           55.25s\n",
      "        94       15207.6362           47.10s\n",
      "        95       15136.6867           39.10s\n",
      "        96       15103.9525           31.08s\n",
      "        97       15024.1589           23.25s\n",
      "        98       14974.4058           15.42s\n",
      "        99       14903.6173            7.69s\n",
      "       100       14873.3077            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.075, loss='deviance', max_depth=14,\n",
       "                           max_features=1.0, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=14, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=5,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = ss()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "# GradientBoostingClassifier\n",
    "\n",
    "GB = GradientBoostingClassifier(n_estimators=100, learning_rate=0.075, max_depth=14,max_features=1.0,min_samples_leaf=14,verbose=5)\n",
    "\n",
    "GB.fit(X, y)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hist GB\n",
    "HGB = HistGradientBoostingClassifier(learning_rate=0.02, loss='categorical_crossentropy', \n",
    "                                               max_depth=8, min_samples_leaf=15)\n",
    "\n",
    "HGB = HGB.fit(X_pca, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.75, max_depth=15,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=100, n_jobs=-1, num_iterations=100, num_leaves=40,\n",
       "               objective='multiclass', random_state=123, reg_alpha=0.0,\n",
       "               reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "               subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lightgbm\n",
    "LGB = LGBMClassifier(objective='multiclass', learning_rate=0.75, num_iterations=100, \n",
    "                     num_leaves=40, random_state=123,max_depth=15)\n",
    "\n",
    "LGB.fit(X_pca, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1       49362.7959           18.43m\n",
      "         2       46578.7639           19.08m\n",
      "         3       44283.0241           19.08m\n",
      "         4       42267.9223           18.93m\n",
      "         5       40506.5750           18.85m\n",
      "         6       38957.2751           18.67m\n",
      "         7       37547.6901           18.56m\n",
      "         8       36278.0766           18.42m\n",
      "         9       35135.1664           18.26m\n",
      "        10       34081.1956           18.10m\n",
      "        11       33118.4529           18.00m\n",
      "        12       32261.1496           17.81m\n",
      "        13       31443.6473           17.62m\n",
      "        14       30715.2200           17.40m\n",
      "        15       30040.0994           17.18m\n",
      "        16       29404.6845           16.97m\n",
      "        17       28811.9159           16.75m\n",
      "        18       28251.1060           16.56m\n",
      "        19       27722.5996           16.37m\n",
      "        20       27232.0737           16.16m\n",
      "        21       26769.3164           15.95m\n",
      "        22       26332.3932           15.74m\n",
      "        23       25951.2269           15.49m\n",
      "        24       25574.4314           15.24m\n",
      "        25       25214.0099           15.00m\n",
      "        26       24882.4331           14.75m\n",
      "        27       24559.4538           14.50m\n",
      "        28       24258.0525           14.24m\n",
      "        29       23965.3212           13.98m\n",
      "        30       23681.6655           13.74m\n",
      "        31       23412.9669           13.48m\n",
      "        32       23174.8313           13.22m\n",
      "        33       22946.8994           12.95m\n",
      "        34       22710.8986           12.69m\n",
      "        35       22532.4124           12.42m\n",
      "        36       22352.1996           12.16m\n",
      "        37       22163.9847           11.90m\n",
      "        38       21954.2899           11.66m\n",
      "        39       21755.1877           11.42m\n",
      "        40       21598.4094           11.16m\n",
      "        41       21432.8902           10.91m\n",
      "        42       21293.5522           10.65m\n",
      "        43       21123.0824           10.41m\n",
      "        44       20977.4185           10.17m\n",
      "        45       20859.0308            9.91m\n",
      "        46       20722.4713            9.68m\n",
      "        47       20604.2953            9.43m\n",
      "        48       20477.0612            9.20m\n",
      "        49       20366.9772            8.97m\n",
      "        50       20235.1675            8.75m\n",
      "        51       20118.2380            8.52m\n",
      "        52       19961.3691            8.32m\n",
      "        53       19822.9746            8.11m\n",
      "        54       19723.6474            7.89m\n",
      "        55       19626.2811            7.67m\n",
      "        56       19537.0172            7.45m\n",
      "        57       19431.6722            7.24m\n",
      "        58       19344.9205            7.03m\n",
      "        59       19255.1413            6.82m\n",
      "        60       19174.0710            6.61m\n",
      "        61       19090.7838            6.40m\n",
      "        62       19010.1079            6.20m\n",
      "        63       18910.2833            6.01m\n",
      "        64       18839.4172            5.81m\n",
      "        65       18762.7611            5.61m\n",
      "        66       18691.5787            5.42m\n",
      "        67       18627.9172            5.22m\n",
      "        68       18537.8425            5.04m\n",
      "        69       18471.1420            4.85m\n",
      "        70       18400.9359            4.67m\n",
      "        71       18323.6054            4.49m\n",
      "        72       18260.2273            4.31m\n",
      "        73       18179.5465            4.14m\n",
      "        74       18100.5318            3.97m\n",
      "        75       18021.1923            3.80m\n",
      "        76       17966.0584            3.62m\n",
      "        77       17907.1803            3.45m\n",
      "        78       17817.7708            3.29m\n",
      "        79       17740.1283            3.13m\n",
      "        80       17635.4625            2.97m\n",
      "        81       17579.0108            2.81m\n",
      "        82       17513.1293            2.65m\n",
      "        83       17430.6379            2.49m\n",
      "        84       17390.5888            2.33m\n",
      "        85       17344.4293            2.17m\n",
      "        86       17272.3886            2.02m\n",
      "        87       17223.1284            1.87m\n",
      "        88       17157.7318            1.72m\n",
      "        89       17108.3679            1.57m\n",
      "        90       17060.9577            1.42m\n",
      "        91       17001.8383            1.27m\n",
      "        92       16936.5749            1.13m\n",
      "        93       16868.3232           58.94s\n",
      "        94       16800.6173           50.38s\n",
      "        95       16765.2164           41.76s\n",
      "        96       16702.4368           33.31s\n",
      "        97       16662.3316           24.88s\n",
      "        98       16618.1365           16.51s\n",
      "        99       16576.9346            8.21s\n",
      "       100       16516.7888            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('HGB',\n",
       "                              HistGradientBoostingClassifier(l2_regularization=0.0,\n",
       "                                                             learning_rate=0.02,\n",
       "                                                             loss='categorical_crossentropy',\n",
       "                                                             max_bins=256,\n",
       "                                                             max_depth=8,\n",
       "                                                             max_iter=100,\n",
       "                                                             max_leaf_nodes=31,\n",
       "                                                             min_samples_leaf=15,\n",
       "                                                             n_iter_no_change=None,\n",
       "                                                             random_state=None,\n",
       "                                                             scoring=None,\n",
       "                                                             tol=1e-07,\n",
       "                                                             validation_fraction=0.1,\n",
       "                                                             verbose=0)),\n",
       "                             ('LGB',\n",
       "                              LGBMClassifier(boosting_type...\n",
       "                                                         max_leaf_nodes=None,\n",
       "                                                         min_impurity_decrease=0.0,\n",
       "                                                         min_impurity_split=None,\n",
       "                                                         min_samples_leaf=14,\n",
       "                                                         min_samples_split=2,\n",
       "                                                         min_weight_fraction_leaf=0.0,\n",
       "                                                         n_estimators=100,\n",
       "                                                         n_iter_no_change=None,\n",
       "                                                         presort='auto',\n",
       "                                                         random_state=None,\n",
       "                                                         subsample=1.0,\n",
       "                                                         tol=0.0001,\n",
       "                                                         validation_fraction=0.1,\n",
       "                                                         verbose=10,\n",
       "                                                         warm_start=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='soft',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "You can combine your best predictors as a VotingClassifier, which can enhance the performance.\n",
    "\n",
    "26/01/2020 - The hist_gradient / lgbm / gradient combination made the best result yet: 80.35%!\n",
    "\"\"\"\n",
    "\n",
    "estimators = [('HGB', HGB), ('LGB', LGB), ('GB', GB)]\n",
    "\n",
    "ensemble = VotingClassifier(estimators, voting='soft')\n",
    "\n",
    "ensemble.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv(\"SubmissionFormat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = sc.transform(X_test_final)\n",
    "submission_df['status_group']=GB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_to_replace = {2:'functional', 1:'functional needs repair', 0:'non functional'}\n",
    "\n",
    "submission_df.status_group = submission_df.status_group.replace(vals_to_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"submission_TatianaSwrt_GB22.csv\",sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible future improvements\n",
    "- Create a 'for' loop to automate the process of model selection\n",
    "- Improve the code to deal with missing values to make it more efficient\n",
    "- More feature ingeneering\n",
    "- try to remove amount_tsh\n",
    "- ? (Balazs) binary -> not unknown but false\n",
    "- log transform to reduce skew: population, amount_tsh\n",
    "- ? quality and rest - don’t make categories\n",
    "- imbalanced -> oversampling with SMOTE -> didn’t help to Balazs\n",
    "- xgboost -> feature importance (not sklearn)\n",
    "- keras to balance classes\n",
    "- try a different scaler\n",
    "\n",
    "After the Feature Engineering Kaggle course I had these ideas:\n",
    "\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
